{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "word-embedding.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNLbiJM1vLP1WdHApDCpkuG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/neildocs/InclusiveGatewayTestCase/blob/master/nlp/word_embedding.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "coeEA9PjIQLB"
      },
      "source": [
        "## Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sfMW2vWpIT3l"
      },
      "source": [
        "There is **TensorFlow Data Services / TFDS** contains many data sets for different categories.\n",
        "\n",
        "\n",
        "One of these data set is **IMDB Review Data Set** which is available at http://ai.stanford.edu/~amaas/data/sentiment/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Oqki8BVOAVW"
      },
      "source": [
        "## Implementation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JboTFOrxOcQk"
      },
      "source": [
        "### Prerequisites"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8LmF1PEnI6rj"
      },
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)\n",
        "\n",
        "# If running on TensorFlow v1.x, need run the below line\n",
        "# tf.enable_eager_execution()\n",
        "\n",
        "# If not using Colab, tensorflow_datasets requires be installed\n",
        "# !pip install -q tensorflow_datasets"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3dJGxDy_PMsw"
      },
      "source": [
        "### Implementation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cVeF3pikOs5b"
      },
      "source": [
        "# Import tfds\n",
        "import tensorflow_datasets as tfds\n",
        "imdb, info = tfds.load(\"imdb_reviews\", with_info=True, as_supervised=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GHKcv1NUPiRi"
      },
      "source": [
        "# Split data set\n",
        "import numpy as np\n",
        "train_data, test_data = imdb['train'], imdb['test']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DUG3yPWpPvj5"
      },
      "source": [
        "# training\n",
        "training_sentences = []\n",
        "training_labels = []\n",
        "\n",
        "for s, l in train_data:\n",
        "    training_sentences.append(str(s.numpy()))\n",
        "    training_labels.append(str(l.numpy()))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6BYcI9zLP23V"
      },
      "source": [
        "# testing\n",
        "testing_sentences = []\n",
        "testing_labels = []\n",
        "\n",
        "for s, l in test_data:\n",
        "    testing_sentences.append(str(s.numpy()))\n",
        "    testing_labels.append(str(l.numpy()))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gCWhG7nBdxNo"
      },
      "source": [
        "training_labels_final = np.array(training_labels).astype(\"int\")\n",
        "testing_labels_final = np.array(testing_labels).astype(\"int\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BlKGTKijeLBm"
      },
      "source": [
        "# Tokenize sentences\n",
        "\n",
        "vocabulary_size = 10000\n",
        "embedding_dim = 16  # 16-dimentional array for vectors\n",
        "max_length = 120\n",
        "trunc_type = \"post\"\n",
        "oov_token = \"<OOV>\"\n",
        "\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "tokenizer = Tokenizer(num_words = vocabulary_size, oov_token = oov_token)\n",
        "tokenizer.fit_on_texts(training_sentences)\n",
        "word_index = tokenizer.word_index\n",
        "\n",
        "sequences = tokenizer.texts_to_sequences(training_sentences)\n",
        "padded = pad_sequences(sequences, maxlen = max_length, truncating = trunc_type)\n",
        "\n",
        "testing_sequences = tokenizer.texts_to_sequences(testing_sentences)\n",
        "testing_padded = pad_sequences(testing_sequences, maxlen = max_length)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uZJ9KjsOfYly"
      },
      "source": [
        "# -------------\n",
        "# Define model\n",
        "# -------------\n",
        "\n",
        "from tensorflow import keras\n",
        "\n",
        "model = keras.Sequential([\n",
        "    keras.layers.Embedding(vocabulary_size, embedding_dim, input_length = max_length),\n",
        "    keras.layers.Flatten(),  # or use 'GlobalAveragePooling1D'\n",
        "    keras.layers.Dense(6, activation = 'relu'),\n",
        "    keras.layers.Dense(1, activation = 'sigmoid')\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ePbRLGylmBqD"
      },
      "source": [
        "model.compile(loss = \"binary_crossentropy\", \n",
        "              optimizer = \"adam\", \n",
        "              metrics = [\"accuracy\"])\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TJLFhVy30Jv3"
      },
      "source": [
        "# ---------\n",
        "# Training \n",
        "# ---------\n",
        "num_epochs = 10\n",
        "model.fit(padded, \n",
        "          training_labels_final, \n",
        "          epochs = num_epochs, \n",
        "          validation_data = (testing_padded, testing_labels_final))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hm94KNbl0iVJ"
      },
      "source": [
        "# ----------------------------\n",
        "# Analyze the embedding layer\n",
        "# ----------------------------\n",
        "embedding_layer = model.layers[0]\n",
        "weights = embedding_layer.get_weights()[0]\n",
        "print(weights.shape)    # shape: (vocabulary_size, embedding_dim)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CgxLKr3I3mkA"
      },
      "source": [
        "# ---------------------------------------------------------------------\n",
        "# To plot it, \n",
        "# \n",
        "# As it currently stands, our word index has the key being the word, \n",
        "# and the value being the token for the word. \n",
        "# \n",
        "# We'll need to flip this around, to look through the padded list \n",
        "# to decode the tokens back into the words\n",
        "# ---------------------------------------------------------------------\n",
        "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9RnFH88045Ou"
      },
      "source": [
        "# ---------------------------------------------------------\n",
        "# Write the vectors and their metadata to files.\n",
        "# TensorFlow use the files to plot the vectors in 3D space\n",
        "# ---------------------------------------------------------\n",
        "import io\n",
        "\n",
        "out_v = io.open('vecs.tsv', 'w', encoding = 'utf-8')    # vector\n",
        "out_m = io.open('meta.tsv', 'w', encoding = 'utf-8')    # metadata\n",
        "\n",
        "for word_num in range(1, vocabulary_size):\n",
        "    word = reverse_word_index[word_num]\n",
        "    embeddings = weights[word_num]\n",
        "    out_m.write(word + '\\n')\n",
        "    # To the vectors file, we simply write out the value of each of the items \n",
        "    # in the array of embeddings, i.e, the co-efficient of each dimension \n",
        "    # on the vector for this word.\n",
        "    out_v.write('\\t'.join([str(x) for x in embeddings]) + '\\n')\n",
        "out_v.close()\n",
        "out_m.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W34XNQKf6o_y"
      },
      "source": [
        "try:\n",
        "    from google.colab import files\n",
        "except ImportError:\n",
        "    pass\n",
        "else:\n",
        "    files.download('vecs.tsv')\n",
        "    files.download('meta.tsv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YWsiWxNY7NBs"
      },
      "source": [
        "Use the downloaded files, and render the results on [TensorFlow Embedding Projector](https://projector.tensorflow.org)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J8lC5tjs7Gk7"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}